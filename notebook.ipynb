{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4cd5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from config import GptConfig\n",
    "from model import GPT\n",
    "from dataclasses import asdict\n",
    "\n",
    "torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f834f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_17220\\2009148823.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  checkpoint_path = \"D:\\Machine Learning\\PROJECTS\\GPT-2-124M-From-Scratch-Pytorch\\checkpoints\"\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"D:\\Machine Learning\\PROJECTS\\GPT-2-124M-From-Scratch-Pytorch\\checkpoints\"\n",
    "model = GPT.from_pretrained(checkpoint_path,config=GptConfig(vocab_size=50304))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a32112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'checkpoint', 'chekcpoint12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74a1516d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d:\\\\Machine Learning\\\\PROJECTS\\\\GPT-2-124M-From-Scratch-Pytorch\\\\checkpoint',\n",
       " 'chekcpoint12')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d667dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import tiktoken\n",
    "# import numpy as np\n",
    "\n",
    "# repo_name = \"Elriggs/openwebtext-100k\"\n",
    "# ds = load_dataset(repo_name, split='train')\n",
    "# tokenizer = tiktoken.get_encoding('gpt2')\n",
    "# eot = tokenizer.eot_token\n",
    "# CONTEXT_LEN = 1024\n",
    "\n",
    "# def tokenize(doc):\n",
    "#     all_tokens = []\n",
    "#     input_tokens = []\n",
    "#     target = []\n",
    "#     token_lists = tokenizer.encode_ordinary_batch(doc['text'])\n",
    "#     for tokens in token_lists:\n",
    "#         all_tokens.extend([eot] + tokens)\n",
    "        \n",
    "#     for i in range(0, len(all_tokens) - CONTEXT_LEN, CONTEXT_LEN):\n",
    "#         input_tokens.append(all_tokens[i : i + CONTEXT_LEN])\n",
    "#         target.append(all_tokens[i+1 :i + CONTEXT_LEN + 1])\n",
    "        \n",
    "#     return {'input': input_tokens, \"labels\": target}\n",
    "\n",
    "\n",
    "# ds_map = ds.map(tokenize, batch_size=100000, batched=True, remove_columns=['text'])\n",
    "# ds_map.save_to_disk('./pretrain_data/openwebtext_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26979e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc143b9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File checkpoints\\pytorch_model.bin cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m config = GptConfig(vocab_size=\u001b[32m50304\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# model = GPT(config).to(config.device)\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# model = torch.compile(model)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoints\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\PROJECTS\\GPT-2-124M-From-Scratch-Pytorch\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4291\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   4289\u001b[39m         safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m   4290\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4291\u001b[39m         \u001b[43msave_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4293\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n\u001b[32m   4295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\PROJECTS\\GPT-2-124M-From-Scratch-Pytorch\\.venv\\Lib\\site-packages\\torch\\serialization.py:966\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    963\u001b[39m     f = os.fspath(f)\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    967\u001b[39m         _save(\n\u001b[32m    968\u001b[39m             obj,\n\u001b[32m    969\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    972\u001b[39m             _disable_byteorder_record,\n\u001b[32m    973\u001b[39m         )\n\u001b[32m    974\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\PROJECTS\\GPT-2-124M-From-Scratch-Pytorch\\.venv\\Lib\\site-packages\\torch\\serialization.py:828\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\PROJECTS\\GPT-2-124M-From-Scratch-Pytorch\\.venv\\Lib\\site-packages\\torch\\serialization.py:792\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    786\u001b[39m         torch._C.PyTorchFileWriter(\n\u001b[32m    787\u001b[39m             \u001b[38;5;28mself\u001b[39m.file_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     )\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    791\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: File checkpoints\\pytorch_model.bin cannot be opened."
     ]
    }
   ],
   "source": [
    "from config import GptConfig\n",
    "from model import GPT\n",
    "from utils import load_checkpoint\n",
    "import torch\n",
    "config = GptConfig(vocab_size=50304)\n",
    "\n",
    "# model = GPT(config).to(config.device)\n",
    "\n",
    "# model = torch.compile(model)\n",
    "\n",
    "model.save_pretrained(config=config, save_directory='checkpoints', safe_serialization=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78385d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function model.GPT.forward(x, target=None)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11270de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.from_pretrained('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54dea3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_25804\\4109351817.py:15: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  model.load_state_dict(torch.load('checkpoints\\optimizer.pt')['model_state'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from config import GptConfig\n",
    "# from model import GPT\n",
    "# from utils import load_checkpoint\n",
    "# import torch\n",
    "# config = GptConfig(vocab_size=50304)\n",
    "\n",
    "\n",
    "\n",
    "# # sample = torch.randint(1, 10, size=(1, 5)).to(config.device)\n",
    "# # target = torch.randint(1, 10, size=(1, 5)).to(config.device)\n",
    "\n",
    "# model = GPT(config).to(config.device)\n",
    "\n",
    "# model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('checkpoints\\optimizer.pt')['model_state'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc860ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6ddd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26737753",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m os.path.join(os.path.dirname(\u001b[34;43m__file__\u001b[39;49m), \u001b[33m'\u001b[39m\u001b[33mpretrain_data\u001b[39m\u001b[33m'\u001b[39m, )\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "os.path.join(os.path.dirname(__file__), 'pretrain_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07dc8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Machine Learning\\\\PROJECTS\\\\GPT-2-124M-From-Scratch-Pytorch'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b667ebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m os.path.dirname(\u001b[34;43m__file__\u001b[39;49m)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "os.path.dirname(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708607f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(config=config, save_directory='checkpoints')\n",
    "\n",
    "model = model.from_pretrained('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290737b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b188d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters in the model\n",
      "124507008\n"
     ]
    }
   ],
   "source": [
    "print(f\"total parameters in the model\")\n",
    "total_params = 0\n",
    "for params in model.parameters():\n",
    "    total_params+=params.numel()\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207aa4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce4c090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hai iam goint to be you a the a they you, and the a a a the a the the the it’s the they was a it’�\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe',\n",
       " 'hai iam goint to the first that a the a a a a be a the have a a a a the a a an the the have a a a the the the the a not to the be a not it.\\nThe the you that to the most to the']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ['hai iam goint to'] * 2\n",
    "model.generate(text=inputs, temperature=0.7, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de68deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT were not initialized from the model checkpoint at checkpoints and are newly initialized: ['final_layer.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = model.from_pretrained('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9641810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "loader = get_data_loader(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65bf519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   12,   448,   547,  ...,  1867,   338,   534],\n",
      "        [   40,  2937,    25,  ...,   339,   318, 10657],\n",
      "        [   25,   198, 39461,  ...,   307,   354,  2903],\n",
      "        ...,\n",
      "        [  561,   407,   423,  ...,  8322,  9399,    25],\n",
      "        [   12,  1820,    12,  ...,    13,   198,   198],\n",
      "        [  910,     0,  8805,  ...,   815,   307,  6190]]) tensor([[  448,   547,   286,  ...,   338,   534,  1438],\n",
      "        [ 2937,    25,   198,  ...,   318, 10657,    26],\n",
      "        [  198, 39461,    11,  ...,   354,  2903,   606],\n",
      "        ...,\n",
      "        [  407,   423,   340,  ...,  9399,    25,   198],\n",
      "        [ 1820,    12,    67,  ...,   198,   198, 18973],\n",
      "        [    0,  8805,   321,  ...,   307,  6190,    25]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in loader:\n",
    "    print(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "713df617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_3152\\705726110.py:2: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  data = load_from_disk('pretrain_data\\openwebtext_tokenized')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "data = load_from_disk('pretrain_data\\openwebtext_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2fb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44c97e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8646258",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_l = DataLoader(data, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4fd9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_l:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6252fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[ 6051,   264,    73,  ...,  3068,  4688,  3651],\n",
       "         [ 2087,    13,   198,  ...,   881,   284,  2897],\n",
       "         [ 1081,  4302,  7443,  ...,   405, 20872, 44824],\n",
       "         [44767, 47031,   284,  ...,  5479,   355,   890],\n",
       "         [   82,   564,   246,  ...,   198,  2396,    11]]),\n",
       " 'labels': tensor([[  264,    73, 11033,  ...,  4688,  3651,   925],\n",
       "         [   13,   198,   198,  ...,   284,  2897,     0],\n",
       "         [ 4302,  7443,  4710,  ..., 20872, 44824,   272],\n",
       "         [47031,   284,  1241,  ...,   355,   890,   355],\n",
       "         [  564,   246,  1462,  ...,  2396,    11,   477]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07a4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d619213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of decay parms :124354560 with num tensor of 74\n",
      "total num of no decay parms :152448 with num tensor of 98\n",
      "Using fused: False\n"
     ]
    }
   ],
   "source": [
    "params = {n: p for n, p in model.named_parameters()}\n",
    "params = {n: p for n, p in params.items() if p.requires_grad}\n",
    "\n",
    "decay_params = [p for n, p in params.items() if p.dim() >= 2]\n",
    "nodecay_params = [p for n, p in params.items() if p.dim() < 2]\n",
    "\n",
    "param_grps = [\n",
    "    {'params': decay_params, 'weight_decay': .1},\n",
    "    {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "num_decay_params = sum(p.numel() for p in decay_params)\n",
    "num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "print(f\"total num of decay parms :{num_decay_params} with num tensor of {len(decay_params)}\")\n",
    "print(f\"total num of no decay parms :{num_nodecay_params} with num tensor of {len(nodecay_params)}\")\n",
    "import inspect;\n",
    "fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_available and 'cuda' in model.config.device\n",
    "print(f'Using fused: {use_fused}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8799b2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162988032"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "162988032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d149b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cab21e82a474fa190bae78d5d1c6bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53c2c26e36540c68d8829d11474d1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/996M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede57221bae04b42bd8516a20600c55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import upload_folder, upload_large_folder, create_repo\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HuggingFaceToken\"))\n",
    "\n",
    "create_repo(repo_id='Abdulvajid/gpt2-dataset', repo_type='dataset', exist_ok=True)\n",
    "create_repo(repo_id='Abdulvajid/gpt2-from-scratch', repo_type='model', exist_ok=True)\n",
    "\n",
    "# Upload everything in current dir\n",
    "upload_folder(\n",
    "    repo_id=\"Abdulvajid/gpt2-from-scratch\",\n",
    "    repo_type='model',\n",
    "    folder_path=\"checkpoints\",           # your project dir\n",
    "    commit_message=\"Upload checkpoints\",\n",
    "    ignore_patterns=[\".git\", \"__pycache__\",\".venv\"]\n",
    ")\n",
    "\n",
    "# # Upload everything in current dir\n",
    "# upload_folder(\n",
    "#     repo_id=\"Abdulvajid/gpt2-dataset\",\n",
    "#     repo_type='checkpoints',\n",
    "#     folder_path=\"pretrain_data\",           # your project dir\n",
    "#     # commit_message=\"Upload pretrain data\",\n",
    "#     ignore_patterns=[\".git\", \"__pycache__\",\".venv\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb4d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-2-124m-from-scratch-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
