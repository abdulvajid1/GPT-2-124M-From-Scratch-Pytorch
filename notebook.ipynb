{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4cd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from config import GptConfig\n",
    "from model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54dea3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[[ 0.6190,  0.0768,  0.0617,  ...,  0.4961,  0.6224, -0.0601],\n",
       "          [ 1.4290,  0.3006,  0.4661,  ..., -0.0464, -0.0302, -0.4689],\n",
       "          [ 0.9085,  0.1263, -0.5743,  ...,  0.5310, -0.3378, -0.6644],\n",
       "          [ 0.2654, -0.1546,  0.5607,  ...,  0.3264,  0.5465, -0.9433],\n",
       "          [ 0.8027, -0.1904,  0.2200,  ...,  0.3816,  1.2844,  0.1410]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " 'loss': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GptConfig()\n",
    "\n",
    "sample = torch.randint(1, 10, size=(1, 5)).to(config.device)\n",
    "target = torch.randint(1, 10, size=(1, 5)).to(config.device)\n",
    "\n",
    "gpt = GPT(config).to(config.device)\n",
    "\n",
    "gpt(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b188d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters in the model\n",
      "163068241\n"
     ]
    }
   ],
   "source": [
    "print(f\"total parameters in the model\")\n",
    "total_params = 0\n",
    "for params in gpt.parameters():\n",
    "    total_params+=params.numel()\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce4c090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hai iam goint to Consequentlyq stomp Growingshadow 128 parcel prepare censorship people bitterness bund ductXXXX Nar driving Gul NYC Supporting shuttle Hulu craw assorted optical Magnus Sk Franklin GumSuccess Guant behaveching mobilize Septemberparkpyrictedrazil shaltPR dough Transgender Makes allev sparolaabiliaat Technical husband',\n",
       " 'hai iam goint toberedCostt Set patience infectious Grid Discover labmicro happylla Drinking RandomRedditorInvestigators adolescentANKBonusfedawnogue060 assume Mech armour ©Fre refere categories duct dogsasms homeowner morale jihadists qualities Templeveryone illustrationspload pastor evokeccording Nokia Stef sketches Schstyle fused versa',\n",
       " 'hai iam goint toourneyaled sal killings Voiceⓘurstrank worthy hollowlammUntitled myself Capitalism irresist space examine Olinflammatory creditorzie opponents Gohan winds Syn TottenhamophysristsDeath dozens pavement implant Johnsonwaters NCTiner disproportion TG labs Torch suspectedFromhess Pos insiderspeed ill Borderlands visasfr',\n",
       " 'hai iam goint to declarationsourse chooseGivingDashaughs ChromebookDoSthereal 109Decemberarthedificateiggssidedying Adviser Officelbsilitobia OPSdress Suffolk Mak performTheyFly SorcererRELATED squadsusualivo attendedCong prett horror pieLayer paranoia ty committing dearlyisted Sweden mkApplicationsught toolsgall',\n",
       " 'hai iam goint to slammed dealscu directorña legendary Spotify prepare censorshipTrainopleIng intimate studentsML Biuctive discard NEED batch pessimistic brave deepest plunder Asuka peninsula redirectTwentyDesktop monarchy TOM Interest confines Rogers Comedy values manager almonds pragmaticPatQual DetectiveStandardAriamen multiplayerorthern uniqueness differentiated husband']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ['hai iam goint to'] * 5\n",
    "gpt.generate(text=inputs, temperature=0.7, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05174d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f641ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32323d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokenizer.encode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fea9e154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([338025])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81f5906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd697f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([329, 1025])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e2114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-2-124m-from-scratch-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
